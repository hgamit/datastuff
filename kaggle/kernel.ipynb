{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "edd91cd6066072d1de67c17020d7d86558b196db"
   },
   "source": [
    "**Notebook Objective:**   Explore data and build base model. \n",
    "\n",
    "**Problem Statement:** [Coreference Resolution] You are provided with the pronoun and two candidate names to which the pronoun could refer. \n",
    "    You must create an algorithm capable of deciding whether the pronoun refers to name A, name B, or neither.\n",
    "    \n",
    "Data Files:\n",
    "* test_stage_1.tsv - the test set data for stage 1\n",
    "* sample_submission_stage_1.csv - a file showing the correct submission format for stage 1\n",
    "\n",
    "Columns:\n",
    "* ID - Unique identifier for an example (Matches to Id in output file format)\n",
    "* Text - Text containing the ambiguous pronoun and two candidate names (about a paragraph in length)\n",
    "* Pronoun - The target pronoun (text)\n",
    "* Pronoun-offset The character offset of Pronoun in Text \n",
    "* A - The first name candidate (text)\n",
    "* A-offset - The character offset of name A in Text\n",
    "* B - The second name candidate\n",
    "* B-offset - The character offset of name B in Text\n",
    "* URL - The URL of the source Wikipedia page for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constants.py', 'CONTRIBUTING.md', 'gap-development.tsv', 'gap-test.tsv', 'gap-validation.tsv', 'gap_scorer.py', 'LICENSE', 'README.md', 'sample_submission_stage_1.csv', 'test_stage_1.tsv', 'test_stage_1.tsv.zip']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./gap\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "* test_stage_1.tsv - the training set\n",
    "* sample_submission_stage_1.csv - submission file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "57b9ca2c9848dfe25f754ae580fc0f4a065bb79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape :  (2000, 9)\n",
      "Submission shape :  (2000, 4)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"./gap/test_stage_1.tsv\", delimiter='\\t').rename(columns={'A': 'A_Noun', 'B': 'B_Noun'})\n",
    "submission = pd.read_csv('./gap/sample_submission_stage_1.csv')\n",
    "\n",
    "print(\"Test shape : \", test_df.shape)\n",
    "print(\"Submission shape : \", submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "9017fd5eefadba69e6e94ab4fd434f695b90f248"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A_Noun</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B_Noun</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2  development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3  development-4  The current members of Crime have also perform...     his   \n",
       "4  development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "\n",
       "   Pronoun-offset             A_Noun  A-offset           B_Noun  B-offset  \\\n",
       "0             274     Cheryl Cassidy       191          Pauline       207   \n",
       "1             284          MacKenzie       228    Bernard Leach       251   \n",
       "2             265            Angeloz       173       De la Sota       246   \n",
       "3             321               Hell       174  Henry Rosenthal       336   \n",
       "4             437  Kitty Oppenheimer       219           Rivera       294   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://en.wikipedia.org/wiki/List_of_Teachers_...  \n",
       "1      http://en.wikipedia.org/wiki/Warren_MacKenzie  \n",
       "2  http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...  \n",
       "3          http://en.wikipedia.org/wiki/Crime_(band)  \n",
       "4        http://en.wikipedia.org/wiki/Jessica_Rivera  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Peak at the data\n",
    "test_df.head()\n",
    "\n",
    "#You are provided with the pronoun and two candidate names to which the pronoun could refer. \n",
    "#You must create an algorithm capable of deciding whether the pronoun refers to name A, name B, or neither."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f0710f6d2f2123fb76a964c06d74cbaf9d9ddaa0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID        A        B  NEITHER\n",
       "0  development-1  0.33333  0.33333  0.33333\n",
       "1  development-2  0.33333  0.33333  0.33333\n",
       "2  development-3  0.33333  0.33333  0.33333\n",
       "3  development-4  0.33333  0.33333  0.33333\n",
       "4  development-5  0.33333  0.33333  0.33333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1/3 probability for each column\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d8e4128a0eedbb6398e756dcb3c6d7a1bec75071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 9 columns):\n",
      "ID                2000 non-null object\n",
      "Text              2000 non-null object\n",
      "Pronoun           2000 non-null object\n",
      "Pronoun-offset    2000 non-null int64\n",
      "A_Noun            2000 non-null object\n",
      "A-offset          2000 non-null int64\n",
      "B_Noun            2000 non-null object\n",
      "B-offset          2000 non-null int64\n",
      "URL               2000 non-null object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 140.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ceddcaa5cb3f84a2b3e8eeaf998288fcef0e433a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                0\n",
       "Text              0\n",
       "Pronoun           0\n",
       "Pronoun-offset    0\n",
       "A_Noun            0\n",
       "A-offset          0\n",
       "B_Noun            0\n",
       "B-offset          0\n",
       "URL               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No null values in the data\n",
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "d8193cd82d7d9c17eaece23963a1981eaef8acda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2454, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting train data from github google repo\n",
    "\n",
    "gh_test = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\", delimiter='\\t')\n",
    "gh_valid = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\", delimiter='\\t')\n",
    "train = pd.concat((gh_test, gh_valid)).rename(columns={'A': 'A_Noun', 'B': 'B_Noun'}).reset_index(drop=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A_Noun</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B_Noun</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text Pronoun  \\\n",
       "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
       "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
       "2  test-3  Though his emigration from the country has aff...      He   \n",
       "3  test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
       "4  test-5  It is about a pair of United States Navy shore...     his   \n",
       "\n",
       "   Pronoun-offset        A_Noun  A-offset  A-coref              B_Noun  \\\n",
       "0             383     Bob Suter       352    False              Dehner   \n",
       "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
       "2             312  Ali Aladhadh       256     True              Saddam   \n",
       "3             526       Alliata       377    False           Pisciotta   \n",
       "4             406         Eddie       421     True         Rock Reilly   \n",
       "\n",
       "   B-offset  B-coref                                             URL  \n",
       "0       366     True      http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
       "1       390    False    http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
       "2       295    False           http://en.wikipedia.org/wiki/Aladhadh  \n",
       "3       536     True  http://en.wikipedia.org/wiki/Gaspare_Pisciotta  \n",
       "4       559    False            http://en.wikipedia.org/wiki/Chasers  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace function\n",
    "\n",
    "def name_replace(s, r1, r2):\n",
    "    s = str(s).replace(r1,r2)\n",
    "    for r3 in r1.split(' '):\n",
    "        s = str(s).replace(r3,r2)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different features Extraction:\n",
    "\n",
    "df = train\n",
    "\n",
    "# minimum offset in a row\n",
    "\n",
    "df['section_min'] = df[['Pronoun-offset', 'A-offset', 'B-offset']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pronoun occurance offset include\n",
    "#df['Pronoun-offset2'] = df['Pronoun-offset'] + df['Pronoun'].map(len)\n",
    "df['A-offset2'] = df['A-offset'] + df['A_Noun'].map(len)\n",
    "df['B-offset2'] = df['B-offset'] + df['B_Noun'].map(len)                               \n",
    "df['section_max'] = df[['Pronoun-offset2', 'A-offset2', 'B-offset2']].max(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_replace(s, r1, r2):\n",
    "    s = str(s).replace(r1,r2)\n",
    "    #first_name and Last_name replace\n",
    "    for r3 in r1.split(' '):\n",
    "        s = str(s).replace(r3,r2)\n",
    "    return s\n",
    "\n",
    "def get_features(df):\n",
    "    df['section_min'] = df[['Pronoun-offset', 'A-offset', 'B-offset']].min(axis=1)\n",
    "    df['Pronoun-offset2'] = df['Pronoun-offset'] + df['Pronoun'].map(len)\n",
    "    df['A-offset2'] = df['A-offset'] + df['A_Noun'].map(len)\n",
    "    df['B-offset2'] = df['B-offset'] + df['B_Noun'].map(len)                               \n",
    "    df['section_max'] = df[['Pronoun-offset2', 'A-offset2', 'B-offset2']].max(axis=1)\n",
    "    #df['Text'] = df.apply(lambda r: r['Text'][: r['Pronoun-offset']] + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis=1)\n",
    "    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['A_Noun'], 'subjectone'), axis=1)\n",
    "    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['B_Noun'], 'subjecttwo'), axis=1)\n",
    "    \n",
    "    \n",
    "    df['A-dist'] = (df['Pronoun-offset'] - df['A-offset']).abs()\n",
    "    df['B-dist'] = (df['Pronoun-offset'] - df['B-offset']).abs()\n",
    "    return(df)\n",
    "\n",
    "train = get_features(train)\n",
    "test = get_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A_Noun</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B_Noun</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "      <th>section_min</th>\n",
       "      <th>Pronoun-offset2</th>\n",
       "      <th>A-offset2</th>\n",
       "      <th>B-offset2</th>\n",
       "      <th>section_max</th>\n",
       "      <th>A-dist</th>\n",
       "      <th>B-dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
       "      <td>352</td>\n",
       "      <td>386</td>\n",
       "      <td>361</td>\n",
       "      <td>372</td>\n",
       "      <td>386</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>433</td>\n",
       "      <td>359</td>\n",
       "      <td>408</td>\n",
       "      <td>433</td>\n",
       "      <td>77</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>314</td>\n",
       "      <td>268</td>\n",
       "      <td>301</td>\n",
       "      <td>314</td>\n",
       "      <td>56</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, subjecttwo said: ``Those who hav...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
       "      <td>377</td>\n",
       "      <td>529</td>\n",
       "      <td>384</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>149</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
       "      <td>406</td>\n",
       "      <td>409</td>\n",
       "      <td>426</td>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>15</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text Pronoun  \\\n",
       "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
       "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
       "2  test-3  Though his emigration from the country has aff...      He   \n",
       "3  test-4  At the trial, subjecttwo said: ``Those who hav...     his   \n",
       "4  test-5  It is about a pair of United States Navy shore...     his   \n",
       "\n",
       "   Pronoun-offset        A_Noun  A-offset  A-coref              B_Noun  \\\n",
       "0             383     Bob Suter       352    False              Dehner   \n",
       "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
       "2             312  Ali Aladhadh       256     True              Saddam   \n",
       "3             526       Alliata       377    False           Pisciotta   \n",
       "4             406         Eddie       421     True         Rock Reilly   \n",
       "\n",
       "   B-offset  B-coref                                             URL  \\\n",
       "0       366     True      http://en.wikipedia.org/wiki/Jeremy_Dehner   \n",
       "1       390    False    http://en.wikipedia.org/wiki/Norberto_Alonso   \n",
       "2       295    False           http://en.wikipedia.org/wiki/Aladhadh   \n",
       "3       536     True  http://en.wikipedia.org/wiki/Gaspare_Pisciotta   \n",
       "4       559    False            http://en.wikipedia.org/wiki/Chasers   \n",
       "\n",
       "   section_min  Pronoun-offset2  A-offset2  B-offset2  section_max  A-dist  \\\n",
       "0          352              386        361        372          386      31   \n",
       "1          353              433        359        408          433      77   \n",
       "2          256              314        268        301          314      56   \n",
       "3          377              529        384        545          545     149   \n",
       "4          406              409        426        570          570      15   \n",
       "\n",
       "   B-dist  \n",
       "0      17  \n",
       "1      40  \n",
       "2      17  \n",
       "3      10  \n",
       "4     153  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading NLP libraries\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading string\n",
    "s = train['Text'][0]\n",
    "w = \"subjectwo\"\n",
    "\n",
    "doc = nlp(str(s))\n",
    "tokens = pd.DataFrame([[token.text, token.dep_] for token in doc], columns=['text', 'dep'])\n",
    "tokens[((tokens['text']==w) & (tokens['dep']=='poss'))]\n",
    "len('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Upon</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>their</td>\n",
       "      <td>poss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acceptance</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>into</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text   dep\n",
       "0        Upon  prep\n",
       "1       their  poss\n",
       "2  acceptance  pobj\n",
       "3        into  prep\n",
       "4         the   det"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Upon their acceptance into the Kontinental Hockey League, subjecttwo left Finland to sign a contract in Germany with EHC M*nchen of the DEL on June 18, 2014. After capturing the German championship with the M*nchen team in 2016, he left the club and was picked up by fellow DEL side EHC Wolfsburg in July 2016. Former NHLer Gary subjectone and Olympic-medalist subjectone are subjecttwo's uncles. His cousin is Minnesota Wild's alternate captain Ryan subjectone.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_features(s, w):\n",
    "    doc = nlp(str(s))\n",
    "    tokens = pd.DataFrame([[token.text, token.dep_] for token in doc], columns=['text', 'dep'])\n",
    "    return len(tokens[((tokens['text']==w) & (tokens['dep']=='poss'))])\n",
    "\n",
    "train['A-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\n",
    "train['B-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))\n",
    "test['A-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\n",
    "test['B-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'A-coref':'A', 'B-coref':'B'})\n",
    "train['A'] = train['A'].astype(int)\n",
    "train['B'] = train['B'].astype(int)\n",
    "train['NEITHER'] = 1.0 - (train['A'] + train['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "train['Pronoun'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Pronoun'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "col = ['Pronoun-offset', 'A-offset', 'B-offset', 'section_min', 'Pronoun-offset2', 'A-offset2', 'B-offset2', 'section_max', 'A-poss', 'B-poss', 'A-dist', 'B-dist']\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[['A', 'B', 'NEITHER']], test_size=0.2, random_state=1)\n",
    "x1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth = 7, n_estimators=1000, random_state=33))\n",
    "# model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33))\n",
    "\n",
    "# param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}\n",
    "# model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**param_dist))\n",
    "\n",
    "model.fit(x1, y1)\n",
    "print('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))\n",
    "model.fit(train[col].fillna(-1), train[['A', 'B', 'NEITHER']])\n",
    "results = model.predict_proba(test[col])\n",
    "test['A'] = results[:,0]\n",
    "test['B'] = results[:,1]\n",
    "test['NEITHER'] = results[:,2]\n",
    "test[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params and tensor varibles\n",
    "\n",
    "learning_rate = 0.3 # loss minimizing steps\n",
    "training_epochs = 1500 # number of iterations to minimize W and b\n",
    "cost_history = np.empty(shape=[1], dtype=float)  # mse values\n",
    "n_dim = x1.shape[1] # Number of columns\n",
    "print(\"ndim:\", n_dim) # total columns for later use in model and parameters creation\n",
    "\n",
    "n_class = 3\n",
    "model_path = \"C:\\\\Users\\\\hmnsh\\\\repos\\\\datastuff\\\\kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer details and neurons for each layer\n",
    "\n",
    "n_hidden_1 = 60\n",
    "n_hidden_2 = 60\n",
    "n_hidden_3 = 60\n",
    "n_hidden_4 = 60\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_dim]) # for each row input\n",
    "y_ = tf.placeholder(tf.float32, [None, n_class]) # for each row output\n",
    "W = tf.Variable(tf.zeros([n_dim, n_class])) #intialized weights to zeros\n",
    "b = tf.Variable(tf.zeros([n_class])) #intialized biases to zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases for each layer\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
    "    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_4, n_class])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_class])),\n",
    "}\n",
    "\n",
    "# Initialize all variables\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "def multilayer_perc(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # hidden layer 1 with sigmoid activation\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # hidden layer 2 with sigmoid activation\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "    # hidden layer 3 with sigmoid activation\n",
    "\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    # hidden layer 4 with relu activation\n",
    "\n",
    "    out_layer = tf.add(tf.matmul(layer_4, weights['out']), biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "# call model\n",
    "\n",
    "y = multilayer_perc(x, weights, biases)\n",
    "\n",
    "# define cost function and optimizer\n",
    "\n",
    "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# A few tiny adjustments for better code readability\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "#warnings.filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost and accuracy - running multilayer_perceptron - training and accuracy\n",
    "\n",
    "mse_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    sess.run(training_step, feed_dict={x: x1, y_: y1})\n",
    "    cost = sess.run(cost_function, feed_dict={x: x1, y_: y1})\n",
    "    cost_history = np.append(cost_history, cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # print( \"Accuracy: \", (sess.run(accuracy, feed_dict={x: test_x, y: test_y} )))\n",
    "    pred_y = sess.run(y, feed_dict={x: x2})\n",
    "    mse = tf.reduce_mean(tf.square(pred_y - y2))\n",
    "    mse_ = sess.run(mse)\n",
    "    mse_history.append(mse_)\n",
    "    accuracy = sess.run(accuracy, feed_dict={x: x1, y_: y1})\n",
    "    accuracy_history.append(accuracy)\n",
    "\n",
    "    print('epoch ', epoch, '-cost ', cost, '-mse', mse, '-Train Accuracy', accuracy)\n",
    "\n",
    "save_path = saver.save(sess, model_path)\n",
    "print('Model Saved in file: %s' % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse and accuracy graph\n",
    "\n",
    "plt.plot(mse_history, 'r')\n",
    "plt.show()\n",
    "plt.plot(accuracy_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final accuracy\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test Accuracy: ', (sess.run(accuracy, feed_dict={x: x2, y_: y2})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final mse\n",
    "\n",
    "pred_y = sess.run(y, feed_dict={x: x2})\n",
    "mse = tf.reduce_mean(tf.square(pred_y - y2))\n",
    "print('MSE: %.4f' % sess.run(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
