{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective Gendered Corefrence Resolution\n",
    "\n",
    "\n",
    "Questions-\n",
    "\n",
    "1. How POS is related to Coreference relations?\n",
    "2. Different parmeters and it's impact on Corefrence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constants.py', 'CONTRIBUTING.md', 'gap-development.tsv', 'gap-test.tsv', 'gap-validation.tsv', 'gap_scorer.py', 'LICENSE', 'README.md', 'sample_submission_stage_1.csv', 'test_stage_1.tsv', 'test_stage_1.tsv.zip']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../gap/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./gap\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2454, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting train data from github google repo\n",
    "\n",
    "gh_test = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\", delimiter='\\t')\n",
    "gh_valid = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\", delimiter='\\t')\n",
    "train = pd.concat((gh_test, gh_valid)).rename(columns={'A': 'A_Noun', 'B': 'B_Noun'}).reset_index(drop=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test data and output sample\n",
    "test_df = pd.read_csv(\"./gap/test_stage_1.tsv\", delimiter='\\t').rename(columns={'A': 'A_Noun', 'B': 'B_Noun'})\n",
    "submission = pd.read_csv('./gap/sample_submission_stage_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(r'gaptrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Upon their acceptance into the Kontinental Hockey League, Dehner left Finland to sign a contract in Germany with EHC M*nchen of the DEL on June 18, 2014. After capturing the German championship with the M*nchen team in 2016, he left the club and was picked up by fellow DEL side EHC Wolfsburg in July 2016. Former NHLer Gary Suter and Olympic-medalist Bob Suter are Dehner's uncles. His cousin is Minnesota Wild's alternate captain Ryan Suter.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['His', 'him', 'He', 'his', 'her', 'She', 'he', 'she', 'Her',\n",
       "       'hers'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique pronouns in train data\n",
    "train['Pronoun'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Capitalized first letter pronouns are starting the sentence.\n",
    "- Subject pronouns - he/she\n",
    "- Object pronouns - him/her\n",
    "- possessive - his/her/hers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get set of entire data to build exploratory template\n",
    "train_sample = train.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A_Noun</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B_Noun</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test-6</td>\n",
       "      <td>The others were Adam Baldwin (Jayne Cobb in Fi...</td>\n",
       "      <td>her</td>\n",
       "      <td>349</td>\n",
       "      <td>Jewel Staite</td>\n",
       "      <td>281</td>\n",
       "      <td>True</td>\n",
       "      <td>Keller</td>\n",
       "      <td>310</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jennifer_Keller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test-7</td>\n",
       "      <td>Allison Fischer (born October 19, 1988) is an ...</td>\n",
       "      <td>She</td>\n",
       "      <td>365</td>\n",
       "      <td>Allison</td>\n",
       "      <td>232</td>\n",
       "      <td>True</td>\n",
       "      <td>Grace Smythe</td>\n",
       "      <td>290</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Allison_Fischer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test-8</td>\n",
       "      <td>The monster arrives and bites Jeni's tongue, b...</td>\n",
       "      <td>her</td>\n",
       "      <td>307</td>\n",
       "      <td>Sophie</td>\n",
       "      <td>248</td>\n",
       "      <td>False</td>\n",
       "      <td>Jeni</td>\n",
       "      <td>277</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Leprechaun:_Origins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test-9</td>\n",
       "      <td>On June 4, 1973 at the Felt Forum, Madison Squ...</td>\n",
       "      <td>he</td>\n",
       "      <td>227</td>\n",
       "      <td>Malave</td>\n",
       "      <td>124</td>\n",
       "      <td>True</td>\n",
       "      <td>Greg Joiner</td>\n",
       "      <td>169</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Edwin_Malave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test-10</td>\n",
       "      <td>Go Away (Lorrie Morgan song) ``Go Away'' is a ...</td>\n",
       "      <td>her</td>\n",
       "      <td>223</td>\n",
       "      <td>Cathy Majeski</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>Lorrie Morgan</td>\n",
       "      <td>154</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Go_Away_(Lorrie_M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test-11</td>\n",
       "      <td>Taken in by S.H.I.E.L.D. she is under the dire...</td>\n",
       "      <td>her</td>\n",
       "      <td>344</td>\n",
       "      <td>Natasha Romanova</td>\n",
       "      <td>313</td>\n",
       "      <td>False</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>369</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Daisy_Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test-12</td>\n",
       "      <td>Neither knows the truth about their respective...</td>\n",
       "      <td>she</td>\n",
       "      <td>290</td>\n",
       "      <td>Dolores</td>\n",
       "      <td>195</td>\n",
       "      <td>False</td>\n",
       "      <td>Rosalinda</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Rosalinda_(Philip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test-13</td>\n",
       "      <td>He was a lawyer in Pulaski County before servi...</td>\n",
       "      <td>His</td>\n",
       "      <td>352</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>206</td>\n",
       "      <td>False</td>\n",
       "      <td>Hubbell</td>\n",
       "      <td>232</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Webster_Hubbell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test-14</td>\n",
       "      <td>He left the Army in 1946. As the recipient of ...</td>\n",
       "      <td>he</td>\n",
       "      <td>256</td>\n",
       "      <td>Alex Bernard</td>\n",
       "      <td>181</td>\n",
       "      <td>False</td>\n",
       "      <td>Hanson</td>\n",
       "      <td>222</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Raymond_Hanson_(c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test-15</td>\n",
       "      <td>Nicola Alexis is a British actress best known ...</td>\n",
       "      <td>She</td>\n",
       "      <td>126</td>\n",
       "      <td>Nicola Alexis</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>WPC Ruby Buxton</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nicola_Alexis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text Pronoun  \\\n",
       "0    test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
       "1    test-2  Between the years 1979-1981, River won four lo...     him   \n",
       "2    test-3  Though his emigration from the country has aff...      He   \n",
       "3    test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
       "4    test-5  It is about a pair of United States Navy shore...     his   \n",
       "5    test-6  The others were Adam Baldwin (Jayne Cobb in Fi...     her   \n",
       "6    test-7  Allison Fischer (born October 19, 1988) is an ...     She   \n",
       "7    test-8  The monster arrives and bites Jeni's tongue, b...     her   \n",
       "8    test-9  On June 4, 1973 at the Felt Forum, Madison Squ...      he   \n",
       "9   test-10  Go Away (Lorrie Morgan song) ``Go Away'' is a ...     her   \n",
       "10  test-11  Taken in by S.H.I.E.L.D. she is under the dire...     her   \n",
       "11  test-12  Neither knows the truth about their respective...     she   \n",
       "12  test-13  He was a lawyer in Pulaski County before servi...     His   \n",
       "13  test-14  He left the Army in 1946. As the recipient of ...      he   \n",
       "14  test-15  Nicola Alexis is a British actress best known ...     She   \n",
       "\n",
       "    Pronoun-offset            A_Noun  A-offset  A-coref              B_Noun  \\\n",
       "0              383         Bob Suter       352    False              Dehner   \n",
       "1              430            Alonso       353     True  Alfredo Di St*fano   \n",
       "2              312      Ali Aladhadh       256     True              Saddam   \n",
       "3              526           Alliata       377    False           Pisciotta   \n",
       "4              406             Eddie       421     True         Rock Reilly   \n",
       "5              349      Jewel Staite       281     True              Keller   \n",
       "6              365           Allison       232     True        Grace Smythe   \n",
       "7              307            Sophie       248    False                Jeni   \n",
       "8              227            Malave       124     True         Greg Joiner   \n",
       "9              223     Cathy Majeski        78    False       Lorrie Morgan   \n",
       "10             344  Natasha Romanova       313    False             Johnson   \n",
       "11             290           Dolores       195    False           Rosalinda   \n",
       "12             352           Clinton       206    False             Hubbell   \n",
       "13             256      Alex Bernard       181    False              Hanson   \n",
       "14             126     Nicola Alexis         0     True     WPC Ruby Buxton   \n",
       "\n",
       "    B-offset  B-coref                                                URL  \n",
       "0        366     True         http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
       "1        390    False       http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
       "2        295    False              http://en.wikipedia.org/wiki/Aladhadh  \n",
       "3        536     True     http://en.wikipedia.org/wiki/Gaspare_Pisciotta  \n",
       "4        559    False               http://en.wikipedia.org/wiki/Chasers  \n",
       "5        310    False       http://en.wikipedia.org/wiki/Jennifer_Keller  \n",
       "6        290    False       http://en.wikipedia.org/wiki/Allison_Fischer  \n",
       "7        277     True   http://en.wikipedia.org/wiki/Leprechaun:_Origins  \n",
       "8        169    False          http://en.wikipedia.org/wiki/Edwin_Malave  \n",
       "9        154     True  http://en.wikipedia.org/wiki/Go_Away_(Lorrie_M...  \n",
       "10       369     True         http://en.wikipedia.org/wiki/Daisy_Johnson  \n",
       "11       207    False  http://en.wikipedia.org/wiki/Rosalinda_(Philip...  \n",
       "12       232     True       http://en.wikipedia.org/wiki/Webster_Hubbell  \n",
       "13       222     True  http://en.wikipedia.org/wiki/Raymond_Hanson_(c...  \n",
       "14        70    False         http://en.wikipedia.org/wiki/Nicola_Alexis  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace function\n",
    "def name_replace(s, r1, r2):\n",
    "    s = str(s).replace(r1,r2)\n",
    "  #  for r3 in r1.split(' '):\n",
    "  #      s = str(s).replace(r3,r2)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def get_features(df):\n",
    "    df['char_count'] = df['Text'].apply(len) \n",
    "    df['word_count'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "    df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
    "    df['punctuation_count'] = df['Text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "    df['title_word_count'] = df['Text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "    df['upper_case_word_count'] = df['Text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "    df['section_min'] = df[['Pronoun-offset', 'A-offset', 'B-offset']].min(axis=1)\n",
    "    df['Pronoun-offset2'] = df['Pronoun-offset'] + df['Pronoun'].map(len)\n",
    "    df['A-offset2'] = df['A-offset'] + df['A_Noun'].map(len)\n",
    "    df['B-offset2'] = df['B-offset'] + df['B_Noun'].map(len)                               \n",
    "    df['section_max'] = df[['Pronoun-offset2', 'A-offset2', 'B-offset2']].max(axis=1)\n",
    "    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['A_Noun'], 'anoun'), axis=1)\n",
    "    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['B_Noun'], 'bnoun'), axis=1)\n",
    "    df['A_count'] = df.Text.str.count('anoun')\n",
    "    df['B_count'] = df.Text.str.count('bnoun')\n",
    "    df['A-dist'] = (df['Pronoun-offset'] - df['A-offset']).abs()\n",
    "    df['B-dist'] = (df['Pronoun-offset'] - df['B-offset']).abs()\n",
    "    return(df)\n",
    "\n",
    "train = get_features(train)\n",
    "test = get_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading NLP libraries\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A_Noun</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B_Noun</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>...</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>section_min</th>\n",
       "      <th>Pronoun-offset2</th>\n",
       "      <th>A-offset2</th>\n",
       "      <th>B-offset2</th>\n",
       "      <th>section_max</th>\n",
       "      <th>A_count</th>\n",
       "      <th>B_count</th>\n",
       "      <th>A-dist</th>\n",
       "      <th>B-dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>352</td>\n",
       "      <td>386</td>\n",
       "      <td>361</td>\n",
       "      <td>372</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>353</td>\n",
       "      <td>433</td>\n",
       "      <td>359</td>\n",
       "      <td>408</td>\n",
       "      <td>433</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>314</td>\n",
       "      <td>268</td>\n",
       "      <td>301</td>\n",
       "      <td>314</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, bnoun said: ``Those who have mad...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>377</td>\n",
       "      <td>529</td>\n",
       "      <td>384</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>149</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>406</td>\n",
       "      <td>409</td>\n",
       "      <td>426</td>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text Pronoun  \\\n",
       "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
       "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
       "2  test-3  Though his emigration from the country has aff...      He   \n",
       "3  test-4  At the trial, bnoun said: ``Those who have mad...     his   \n",
       "4  test-5  It is about a pair of United States Navy shore...     his   \n",
       "\n",
       "   Pronoun-offset        A_Noun  A-offset  A-coref              B_Noun  \\\n",
       "0             383     Bob Suter       352    False              Dehner   \n",
       "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
       "2             312  Ali Aladhadh       256     True              Saddam   \n",
       "3             526       Alliata       377    False           Pisciotta   \n",
       "4             406         Eddie       421     True         Rock Reilly   \n",
       "\n",
       "   B-offset  B-coref  ... upper_case_word_count  section_min  Pronoun-offset2  \\\n",
       "0       366     True  ...                     4          352              386   \n",
       "1       390    False  ...                     0          353              433   \n",
       "2       295    False  ...                     1          256              314   \n",
       "3       536     True  ...                     1          377              529   \n",
       "4       559    False  ...                     1          406              409   \n",
       "\n",
       "   A-offset2  B-offset2  section_max  A_count  B_count  A-dist  B-dist  \n",
       "0        361        372          386        1        2      31      17  \n",
       "1        359        408          433        2        1      77      40  \n",
       "2        268        301          314        2        1      56      17  \n",
       "3        384        545          545        3        2     149      10  \n",
       "4        426        570          570        2        1      15     153  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_features(s, w):\n",
    "    doc = nlp(str(s))\n",
    "    tokens = pd.DataFrame([[token.text, token.dep_] for token in doc], columns=['text', 'dep'])\n",
    "    return len(tokens[((tokens['text']==w) & (tokens['dep']=='poss'))])\n",
    "\n",
    "train['A-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'anoun'))\n",
    "train['B-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'bnoun'))\n",
    "test['A-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'anoun'))\n",
    "test['B-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'bnoun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'A-coref':'A', 'B-coref':'B'})\n",
    "train['A'] = train['A'].astype(int)\n",
    "train['B'] = train['B'].astype(int)\n",
    "train['NEITHER'] = 1.0 - (train['A'] + train['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>section_min</th>\n",
       "      <th>Pronoun-offset2</th>\n",
       "      <th>A_count</th>\n",
       "      <th>B_count</th>\n",
       "      <th>A-offset2</th>\n",
       "      <th>B-offset2</th>\n",
       "      <th>section_max</th>\n",
       "      <th>A-poss</th>\n",
       "      <th>B-poss</th>\n",
       "      <th>A-dist</th>\n",
       "      <th>B-dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>544</td>\n",
       "      <td>93</td>\n",
       "      <td>5.787234</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>379</td>\n",
       "      <td>451</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>385</td>\n",
       "      <td>401</td>\n",
       "      <td>451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>516</td>\n",
       "      <td>80</td>\n",
       "      <td>6.370370</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>315</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>329</td>\n",
       "      <td>358</td>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>456</td>\n",
       "      <td>78</td>\n",
       "      <td>5.772152</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>297</td>\n",
       "      <td>352</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>334</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>376</td>\n",
       "      <td>62</td>\n",
       "      <td>5.968254</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>265</td>\n",
       "      <td>285</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>444</td>\n",
       "      <td>74</td>\n",
       "      <td>5.920000</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>336</td>\n",
       "      <td>433</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>340</td>\n",
       "      <td>385</td>\n",
       "      <td>433</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_count  word_count  word_density  punctuation_count  \\\n",
       "1699         544          93      5.787234                 15   \n",
       "1790         516          80      6.370370                 11   \n",
       "1665         456          78      5.772152                 15   \n",
       "898          376          62      5.968254                 11   \n",
       "270          444          74      5.920000                 21   \n",
       "\n",
       "      title_word_count  upper_case_word_count  section_min  Pronoun-offset2  \\\n",
       "1699                24                      4          379              451   \n",
       "1790                23                      3          315              377   \n",
       "1665                 9                      1          297              352   \n",
       "898                 13                      0          262              302   \n",
       "270                 16                      1          336              433   \n",
       "\n",
       "      A_count  B_count  A-offset2  B-offset2  section_max  A-poss  B-poss  \\\n",
       "1699        1        1        385        401          451       0       0   \n",
       "1790        1        2        329        358          377       0       2   \n",
       "1665        1        2        306        334          352       0       0   \n",
       "898         3        3        265        285          302       0       1   \n",
       "270         1        2        340        385          433       0       1   \n",
       "\n",
       "      A-dist  B-dist  \n",
       "1699      69      58  \n",
       "1790      59      28  \n",
       "1665      52      20  \n",
       "898       37      18  \n",
       "270       95      52  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "col = ['char_count','word_count','word_density','punctuation_count','title_word_count','upper_case_word_count','section_min', 'Pronoun-offset2', 'A_count', 'B_count', 'A-offset2', 'B-offset2', 'section_max', 'A-poss', 'B-poss', 'A-dist', 'B-dist']\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[['A', 'B', 'NEITHER']], test_size=0.2, random_state=1)\n",
    "x1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  B  NEITHER\n",
       "1699  1  0      0.0\n",
       "1790  1  0      0.0\n",
       "1665  1  0      0.0\n",
       "898   1  0      0.0\n",
       "270   0  1      0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth = 7, n_estimators=1000, random_state=33))\n",
    "# model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33))\n",
    "\n",
    "# param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}\n",
    "# model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**param_dist))\n",
    "\n",
    "model.fit(x1, y1)\n",
    "\n",
    "\n",
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40990296, 0.38521437, 0.15524851],\n",
       "       [0.55801026, 0.34676216, 0.08010698],\n",
       "       [0.56544831, 0.28365129, 0.2957617 ],\n",
       "       ...,\n",
       "       [0.43607137, 0.38007422, 0.14192504],\n",
       "       [0.7292482 , 0.20795145, 0.08734859],\n",
       "       [0.65349419, 0.3029094 , 0.07773353]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  B  NEITHER\n",
       "120   0  0      1.0\n",
       "768   0  1      0.0\n",
       "521   0  0      1.0\n",
       "107   1  0      0.0\n",
       "1652  1  0      0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss 0.8593135031612028\n"
     ]
    }
   ],
   "source": [
    "print('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss 0.8593135031612028\n"
     ]
    }
   ],
   "source": [
    "model = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth = 7, n_estimators=1000, random_state=33))\n",
    "# model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33))\n",
    "\n",
    "# param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}\n",
    "# model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**param_dist))\n",
    "\n",
    "model.fit(x1, y1)\n",
    "print('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))\n",
    "model.fit(train[col].fillna(-1), train[['A', 'B', 'NEITHER']])\n",
    "results = model.predict_proba(test[col])\n",
    "test['A'] = results[:,0]\n",
    "test['B'] = results[:,1]\n",
    "test['NEITHER'] = results[:,2]\n",
    "test[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(x1, x2, y1, y2):\n",
    "    model.fit(x1, y1)\n",
    "    log_loss = metrics.log_loss(y2, model.predict_proba(x2))\n",
    "    print('log_loss', log_loss)\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build step forward feature selection\n",
    "sfs1 = sfs(model,\n",
    "           k_features=5,\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "# Perform SFFS\n",
    "sfs1 = sfs1.fit(x1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features?\n",
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['B_count', 'A-poss', 'B-poss']\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[['A', 'B', 'NEITHER']], test_size=0.2, random_state=1)\n",
    "\n",
    "model.fit(x1, y1)\n",
    "print('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))\n",
    "model.fit(train[col].fillna(-1), train[['A', 'B', 'NEITHER']])\n",
    "results = model.predict_proba(test[col])\n",
    "test['A'] = results[:,0]\n",
    "test['B'] = results[:,1]\n",
    "test['NEITHER'] = results[:,2]\n",
    "test[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss 0.9750405067460357\n"
     ]
    }
   ],
   "source": [
    "col = ['A-poss', 'B-poss']\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[['A', 'B', 'NEITHER']], test_size=0.2, random_state=1)\n",
    "\n",
    "#model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33))\n",
    "\n",
    "param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}\n",
    "model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**param_dist))\n",
    "\n",
    "model.fit(x1, y1)\n",
    "print('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))\n",
    "model.fit(train[col].fillna(-1), train[['A', 'B', 'NEITHER']])\n",
    "results = model.predict_proba(test[col])\n",
    "test['A'] = results[:,0]\n",
    "test['B'] = results[:,1]\n",
    "test['NEITHER'] = results[:,2]\n",
    "test[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Let's pickle it for later use\n",
    "train.to_pickle(\"train.pkl\")\n",
    "test.to_pickle(\"test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
