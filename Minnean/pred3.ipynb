{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T13:45:07.772446Z",
     "start_time": "2019-11-11T13:44:58.855847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 13)\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/\n",
    "#https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "\n",
    "\n",
    "# univariate multi-step vector-output stacked lstm example\n",
    "# Importing the libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional,Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2\n",
    "from numpy import hstack\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "def plot_eval(real_stock_price, predicted_stock_price):\n",
    "    print(\"sqrt mean_squared_error: \", sqrt(mean_squared_error(real_stock_price, predicted_stock_price)))\n",
    "    print(\"mean_squared_error: \", mean_squared_error(real_stock_price, predicted_stock_price))\n",
    "    mean_absolute_er = mean_absolute_error(real_stock_price, predicted_stock_price)\n",
    "    print(\"mean_absolute_error: \", mean_absolute_er)\n",
    "    \n",
    "    #real_stock_price, predicted_stock_price\n",
    "    SS_Residual = sum((real_stock_price-predicted_stock_price)**2)\n",
    "    SS_Total = sum((real_stock_price-np.mean(real_stock_price))**2)\n",
    "    r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "    adjusted_r_squared = 1 - (1-r_squared)*(len(real_stock_price)-1)/(len(real_stock_price)-X_train.shape[1]-1)\n",
    "    print (\"R Squared:\", r_squared, \"\\nAdjusted R Squared:\", adjusted_r_squared)\n",
    "    out_arr = np.subtract(predicted_stock_price, real_stock_price)\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    # Visualising the results\n",
    "    plt.plot(real_stock_price, color = 'red',  marker='o', label = 'Real Stock Price')\n",
    "    plt.plot(predicted_stock_price, color = 'blue',  marker='o', label = 'Predicted Stock Price')\n",
    "    plt.title('Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def add_datepart(df, fldnames, drop=False, time=False, errors=\"raise\"):\t\n",
    "    if isinstance(fldnames,str):\n",
    "        fldnames = [fldnames]\n",
    "    for fldname in fldnames:\n",
    "        fld = df[fldname]\n",
    "        fld_dtype = fld.dtype\n",
    "        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            fld_dtype = np.datetime64\n",
    "        if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "        #targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        for n in attr: df[n] = getattr(fld.dt, n.lower())\n",
    "        df['Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "        if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "    df.drop(['Elapsed','Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start'], axis=1, inplace=True)\n",
    "        \n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def calc_feat(sequence, n_steps_in, n_steps_out):\n",
    "    X= []\n",
    "    k = 0 # row itreator\n",
    "    size = len(sequence)\n",
    "    for i in range(n_steps_in, size):\n",
    "        vwap = 0\n",
    "        vol = 0\n",
    "        vchg = 0\n",
    "        for j in range(n_steps_in):\n",
    "            if (k<i):\n",
    "#                print(\"i:\", i,\" j:\",j)\n",
    "#                print(\"High, Low, Close\", sequence[k, 0:3])\n",
    "#                print(\"Vol\", sequence[k, 4])\n",
    "#                print(\"Change\", sequence[k, 3])\n",
    "                vwap += (((np.sum(sequence[k, 0:3]))/3) * sequence[k, 4])\n",
    "                vchg += (((sequence[k, 3])) * sequence[k, 4])\n",
    "                vol += sequence[k, 4]\n",
    "                k = k+1\n",
    " #               print(\"vwap:\", vwap, \" vchg\", vchg)\n",
    "        #if vwap !=0  and vol != 0:\n",
    "        \n",
    "        X = np.append(X, vwap/vol) #vwap\n",
    "        X = np.append(X, vchg/vol) #Change\n",
    "        #X = np.append(X, sequence[i, 4]) #volume\n",
    "        #X = np.append(X, sequence[i, 5]) #openInt\n",
    "        #X = np.append(X, sequence[i, 6]) #DTWEXB\n",
    "        #X = np.append(X, sequence[i, 7]) #Year\n",
    "        #X = np.append(X, sequence[i, 8]) #Mnt\n",
    "        #X = np.append(X, sequence[i, 9]) #Week\n",
    "        #X = np.append(X, sequence[i, 10]) #DayofMonth\n",
    "        #X = np.append(X, sequence[i, 11]) #Dayofweek\n",
    "        #X = np.append(X, sequence[i, 12]) #Dayofyear\n",
    "    \n",
    "    num_of_features = 2 #num_of_features\n",
    "    sz = size-(n_steps_in)\n",
    "    X = np.array(X.reshape(sz, num_of_features)) \n",
    "    #X = X[:-(n_steps_out), :]\n",
    "    return X\n",
    "\n",
    "\n",
    "# load dataset\n",
    "# Importing the training set\n",
    "dataset_train = pd.read_csv('C:/Users/hmnsh/repos/datastuff/Minnean/marketdata/zsh20_daily_price-history-11-11-2019.csv') #, nrows=10\n",
    "#dataset_train = dataset_train[dataset_train[\"Volume\"] != 0]\n",
    "\n",
    "#dataset_dxy = pd.read_csv('C:/Users/hmnsh/repos/datastuff/Minnean/marketdata/DTWEXB.csv')\n",
    "#dataset_train = pd.merge(dataset_train, dataset_dxy,  left_on='Time', right_on='Time', how='left')\n",
    "\n",
    "dataset_train.Time = pd.to_datetime(dataset_train.Time.str.replace('D', 'T'))\n",
    "dataset_train = dataset_train.sort_values('Time')\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 2, 3\n",
    "\n",
    "add_datepart(dataset_train, 'Time')\n",
    "\n",
    "dataset_train.set_index('Time', inplace=True)\n",
    "print(dataset_train.shape)\n",
    "\n",
    "# define input sequence\n",
    "training_feat = dataset_train.iloc[:, 1:15].values\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (-1, 1))\n",
    "training_feat_scaled = sc.fit_transform(training_feat)\n",
    "\n",
    "#X_feat = calc_feat(training_feat, n_steps_in, n_steps_out)\n",
    "X_feat = calc_feat(training_feat_scaled, n_steps_in, n_steps_out)\n",
    "\n",
    "# define input sequence\n",
    "training_set = dataset_train.iloc[:, 3:4].values\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (-1, 1))\n",
    "raw_seq = sc.fit_transform(training_set)\n",
    "\n",
    "# split into samples\n",
    "X_whole, y_whole = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "\n",
    "# summarize the data\n",
    "#for i in range(len(X_whole)):\n",
    "#\tprint(X_whole[i], y_whole[i])\n",
    "\n",
    "X_whole = X_whole.reshape(X_whole.shape[0], X_whole.shape[1])\n",
    "y_whole = y_whole.reshape(y_whole.shape[0], y_whole.shape[1])\n",
    "\n",
    "#step2 model - drop 1st row\n",
    "n_steps_out = 1\n",
    "pred_seq = y_whole[-1,2:3]#.reshape(-1, 1)\n",
    "pred_feat = X_feat[X_whole.shape[0], :]\n",
    "pred_seq = np.concatenate((pred_seq , pred_feat), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T13:45:26.239030Z",
     "start_time": "2019-11-11T13:45:26.231029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_whole = y_whole[:,2:3]\n",
    "trp_rows = X_feat.shape[0] - X_whole.shape[0]\n",
    "if trp_rows>0:\n",
    "    \n",
    "    X_feat = X_feat[:-(trp_rows), :]\n",
    "#Concate features\n",
    "X_whole = hstack((X_whole, X_feat))\n",
    "print(X_whole.shape)\n",
    "\n",
    "#Train -valid and Test split in time order\n",
    "X_train = X_whole[0:362,:].copy()\n",
    "X_valid = X_whole[362:420,:].copy()\n",
    "X_test = X_whole[420:,:].copy()\n",
    "\n",
    "y_train = y_whole[0:362].copy()\n",
    "y_valid = y_whole[362:420].copy()\n",
    "y_test = y_whole[420:].copy()\n",
    "\n",
    "Xf_whole = X_whole.copy()\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "Xf_whole = Xf_whole.reshape((Xf_whole.shape[0], Xf_whole.shape[1], n_features))\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(Xf_whole.shape[1], n_features)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=True,bias_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, return_sequences=True,bias_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(10,bias_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[\"mean_squared_error\"])\n",
    "# fit model , batch_size=32\n",
    "hist = model.fit(Xf_whole, y_whole, epochs=1500, verbose=0, batch_size=96)\n",
    "# demonstrate prediction\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.suptitle('Training Evaluation', fontsize=24)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "#Plotting Training history\n",
    "print(hist.history.keys())\n",
    "\n",
    "# Visualising the results\n",
    "plt.plot(hist.history['loss'], color = 'blue',  label = 'train_loss')\n",
    "#plt.plot(hist.history['val_loss'], color = 'red',  label = 'val_loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.subplot(2,2,2)\n",
    "# Visualising the results\n",
    "plt.plot(hist.history['mean_squared_error'], color = 'blue',  label = 'train_mean_squared_error')\n",
    "##plt.plot(hist.history['val_mean_squared_error'], color = 'red',  label = 'val_mean_squared_error')\n",
    "plt.title('mean_squared_error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mean_squared_error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# demonstrate prediction\n",
    "#yhat = model.predict(X_test, verbose=0)\n",
    "\n",
    "#for col in range(yhat.shape[1]):\n",
    "#    print(\"Day Prediction (Step): \", col+1)\n",
    "#    yhat[:,col:col+1] = sc.inverse_transform(yhat[:,col:col+1])\n",
    "#    y_test[:,col:col+1] = sc.inverse_transform(y_test[:,col:col+1])\n",
    "#    plot_eval(y_test[:,col:col+1],yhat[:,col:col+1])\n",
    "\n",
    "## lstm pred\n",
    "\n",
    "pred_seq_reshape = pred_seq.reshape(1, 3, 1)\n",
    "pred_lstm =  model.predict(pred_seq_reshape, verbose=0)\n",
    "\n",
    "#XGBoost\n",
    "\n",
    "#Train -valid and Test split in time order\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "reg=XGBRegressor(learning_rate=0.01, n_estimators=1500, n_jobs=1)\n",
    "reg.fit(X_whole, y_whole)\n",
    "\n",
    "pred_reg= reg.predict(pred_seq.reshape(1, 3))\n",
    "\n",
    "pred_final = (pred_lstm+pred_reg)/2\n",
    "predicted_stock_price = sc.inverse_transform(pred_final.reshape(-1, 1))\n",
    "\n",
    "pred_zsk = predicted_stock_price + 7.028119\n",
    "pred_zsn = predicted_stock_price + 14.62014\n",
    "\n",
    "print(\"ZSH, ZSK, ZSN\", predicted_stock_price, pred_zsk, pred_zsn )\n",
    "\n",
    "###ZSH 939.532, ZSK 7.028119, ZSN 14.62014\n",
    "###15ZSH, ZSK, ZSN [[939.5322]] [[946.56036]] [[954.15234]]\n",
    "###13ZSH, ZSK, ZSN [[943.0223]] [[950.0504]] [[957.6424]]\n",
    "###12ZSH, ZSK, ZSN [[937.87463]] [[944.9028]] [[952.49475]]\n",
    "###11ZSH, ZSK, ZSN [[930.36127]] [[937.3894]] [[944.9814]]\n",
    "###10ZSH, ZSK, ZSN [[937.217]] [[944.2451]] [[951.8371]]\n",
    "##9ZSH, ZSK, ZSN [[937.94794]] [[944.9761]] [[952.56805]]\n",
    "##8ZSH, ZSK, ZSN [[937.5538]] [[944.5819]] [[952.1739]]\n",
    "##7ZSH, ZSK, ZSN [[938.8137]] [[945.84186]] [[953.43384]]\n",
    "##6ZSH, ZSK, ZSN [[940.06146]] [[947.0896]] [[954.6816]]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
